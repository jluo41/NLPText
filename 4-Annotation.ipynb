{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical NER Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/clinical_ner_sample/MEntity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.610 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 659\n",
      "Total Num of Unique Tokens 245\n",
      "CORPUS\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 3\n",
      "SENT\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 21\n",
      "TOKEN\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 659\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/clinical_ner_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 245\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/clinical_ner_sample/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.Entity',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 1, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedPOS Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/medical_pos_sample/batch2\n",
      "4\n",
      "patient5218-sent4.UMLSTag\n",
      "corpus/medical_pos_sample/batch1\n",
      "2\n",
      "patient5175-sent2.UMLSTag\n",
      "10\n",
      "patient4857-sent10.UMLSTag\n",
      "Total Num of All    Tokens 6460\n",
      "Total Num of Unique Tokens 713\n",
      "CORPUS\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 20\n",
      "SENT\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 179\n",
      "TOKEN\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 6460\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 153\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/medical_pos_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 713\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/medical_pos_sample/' # TODO\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'annofile4sent'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.UMLSTag',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/boson/bosonNER.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.589 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 533491\n",
      "Total Num of Unique Tokens 3825\n",
      "CORPUS\tit is Dumped into file: data/boson/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/boson/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/boson/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tit is Dumped into file: data/boson/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10214\n",
      "TOKEN\tit is Dumped into file: data/boson/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 533491\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/boson/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/boson/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/boson/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 25\n",
      "\t\tWrite to: data/boson/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/boson/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3825\n",
      "\t\tWrite to: data/boson/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'anno_embed_in_text'\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'记 者 从 杭 州 江 干 区 公 安 分 局 了 解 到 , 经 过 一 个 多 月 的 侦 查 工 作 , 江 干 区 禁 毒 专 案 组 抓 获 吸 贩 毒 人 员 5 名 , 缴 获 “ 冰 毒 ” 4 0 0 余 克 , 毒 资 3 0 0 0 0 余 元 , 扣 押 汽 车 一 辆 。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 2\n",
    "\n",
    "st = Sentence(locidx)\n",
    "\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'org_name-B',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-E',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'org_name-B',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-E',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product_name': 4109,\n",
       " 'time': 4243,\n",
       " 'person_name': 5123,\n",
       " 'org_name': 2683,\n",
       " 'location': 4593,\n",
       " 'company_name': 2368}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/ResumeCN/test.char.bmes\n",
      "corpus/ResumeCN/train.char.bmes\n",
      "corpus/ResumeCN/dev.char.bmes\n",
      "Total Num of All    Tokens 149123\n",
      "Total Num of Unique Tokens 1865\n",
      "CORPUS\tit is Dumped into file: data/ResumeCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/ResumeCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/ResumeCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4476\n",
      "SENT\tit is Dumped into file: data/ResumeCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4476\n",
      "TOKEN\tit is Dumped into file: data/ResumeCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 149123\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 33\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/ResumeCN/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1865\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/ResumeCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.bmes'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NAME': 1048,\n",
       " 'PRO': 337,\n",
       " 'EDU': 1071,\n",
       " 'TITLE': 7762,\n",
       " 'ORG': 5684,\n",
       " 'CONT': 320,\n",
       " 'RACE': 143,\n",
       " 'LOC': 55}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NewsCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T08:21:49.402497Z",
     "start_time": "2019-08-18T08:21:45.917868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NewsCN/demo.dev.char\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.619 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/NewsCN/demo.test.char\n",
      "corpus/NewsCN/demo.train.char\n",
      "Total Num of All    Tokens 69463\n",
      "Total Num of Unique Tokens 2570\n",
      "CORPUS\tit is Dumped into file: data/NewsCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NewsCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/NewsCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1536\n",
      "SENT\tit is Dumped into file: data/NewsCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1536\n",
      "TOKEN\tit is Dumped into file: data/NewsCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 69463\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/NewsCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NewsCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/NewsCN/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2570\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NewsCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.char'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NewsCN/demo.dev.char\n",
      "corpus/NewsCN/demo.test.char\n",
      "corpus/NewsCN/demo.train.char\n",
      "Total Num of All    Tokens 54635\n",
      "Total Num of Unique Tokens 2329\n",
      "CORPUS\tit is Dumped into file: data/NewsCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NewsCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/NewsCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1129\n",
      "SENT\tit is Dumped into file: data/NewsCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1129\n",
      "TOKEN\tit is Dumped into file: data/NewsCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 54635\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/NewsCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NewsCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/NewsCN/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2329\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NewsCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.char'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'GPE': 1488, 'ORG': 626, 'PER': 698, 'LOC': 210}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSRA-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T13:31:47.183093Z",
     "start_time": "2019-08-18T13:31:36.396767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/MSRA/testright1.txt\n",
      "corpus/MSRA/train1.txt\n",
      "Total Num of All    Tokens 2339922\n",
      "Total Num of Unique Tokens 4831\n",
      "CORPUS\tit is Dumped into file: data/MSRA/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/MSRA/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/MSRA/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 50725\n",
      "SENT\tit is Dumped into file: data/MSRA/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 50725\n",
      "TOKEN\tit is Dumped into file: data/MSRA/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 2339922\n",
      "**************************************** \n",
      "\n",
      "annoE-bioes\tis Dumped into file: data/MSRA/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 13\n",
      "\t\tWrite to: data/MSRA/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/MSRA/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 4831\n",
      "\t\tWrite to: data/MSRA/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/MSRA/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = []\n",
    "\n",
    "# 其实/o 非/o 汉/o 非/o 唐/o ，/o 又是/o 什么/o 与/o 什么/o 呢/o ？/o \n",
    "anno = 'anno_embed_along_token' \n",
    "anno_keywords = {\n",
    "    'sep_between_tokens': ' ',\n",
    "    'sep_between_token_label': '/', \n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T13:31:57.821940Z",
     "start_time": "2019-08-18T13:31:54.520594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nt': 21900, 'nr': 19588, 'ns': 39394}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    if len(SSETs) == 0:\n",
    "        zlocidx = locidx\n",
    "    \n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T13:32:02.749608Z",
     "start_time": "2019-08-18T13:32:02.735130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train valid test from corpus itself\n",
      "The num of train sentences: 46364\n",
      "The num of test  sentences: 4361\n",
      "The num of valid sentences: 0\n"
     ]
    }
   ],
   "source": [
    "from nlptext.folder import Folder\n",
    "data = []\n",
    "for data_type in ['train', 'test', 'dev']:\n",
    "    t = []\n",
    "    for i in range(BasicObject.GROUP['length']):\n",
    "        f = Folder(i)\n",
    "        if data_type in f.name:\n",
    "            t = list(range(*f.IdxSentStartEnd))\n",
    "    data.append(t)\n",
    "    \n",
    "    \n",
    "train_sent_idx, test_sent_idx, valid_sent_idx = data\n",
    "print('load train valid test from corpus itself')\n",
    "print('The num of train sentences:', len(train_sent_idx))\n",
    "print('The num of test  sentences:', len(test_sent_idx))\n",
    "print('The num of valid sentences:', len(valid_sent_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/CoNLL-2003/eng.train.openNLP\n",
      "corpus/CoNLL-2003/eng.testb.openNLP\n",
      "corpus/CoNLL-2003/eng.testa.openNLP\n",
      "Total Num of All    Tokens 254708\n",
      "Total Num of Unique Tokens 27270\n",
      "CORPUS\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 16477\n",
      "SENT\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16477\n",
      "TOKEN\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254708\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/CoNLL-2003/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/pos_en-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/CoNLL-2003/word/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/CoNLL-2003/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 27270\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/CoNLL-2003/'\n",
    "\n",
    "Corpus2GroupMethod = '.openNLP'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' ' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ', # the seperation\n",
    "    'connector': ' ', \n",
    "    'suffix': False,\n",
    "    'change_tags': True, # change I-B tags\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares on the Dhaka Stock Exchange ( DSE ) may remain steady as small investors are expected to target mainly blue chips while overseas investors will prefer to keep to the sidelines when the market reopens after Moslem Friday weekend , brokers said .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Shares', 'O'),\n",
       " ('on', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Dhaka', 'ORG-B'),\n",
       " ('Stock', 'ORG-I'),\n",
       " ('Exchange', 'ORG-I'),\n",
       " ('(', 'O'),\n",
       " ('DSE', 'ORG-B'),\n",
       " (')', 'O'),\n",
       " ('may', 'O'),\n",
       " ('remain', 'O'),\n",
       " ('steady', 'O'),\n",
       " ('as', 'O'),\n",
       " ('small', 'O'),\n",
       " ('investors', 'O'),\n",
       " ('are', 'O'),\n",
       " ('expected', 'O'),\n",
       " ('to', 'O'),\n",
       " ('target', 'O'),\n",
       " ('mainly', 'O'),\n",
       " ('blue', 'O'),\n",
       " ('chips', 'O'),\n",
       " ('while', 'O'),\n",
       " ('overseas', 'O'),\n",
       " ('investors', 'O'),\n",
       " ('will', 'O'),\n",
       " ('prefer', 'O'),\n",
       " ('to', 'O'),\n",
       " ('keep', 'O'),\n",
       " ('to', 'O'),\n",
       " ('the', 'O'),\n",
       " ('sidelines', 'O'),\n",
       " ('when', 'O'),\n",
       " ('the', 'O'),\n",
       " ('market', 'O'),\n",
       " ('reopens', 'O'),\n",
       " ('after', 'O'),\n",
       " ('Moslem', 'MISC-B'),\n",
       " ('Friday', 'O'),\n",
       " ('weekend', 'O'),\n",
       " (',', 'O'),\n",
       " ('brokers', 'O'),\n",
       " ('said', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "st = Sentence(16471)\n",
    "print(st.sentence)\n",
    "list(zip(st.sentence.split(' '), st.get_stored_hyperstring('annoE', tagScheme= 'BIO')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ORG': 10496, 'MISC': 5248, 'PER': 10990, 'LOC': 10944}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# from nlptext.base import BasicObject\n",
    "\n",
    "# CORPUSPath = 'corpus/ResumeEN/'\n",
    "\n",
    "# Corpus2GroupMethod = '.json'\n",
    "\n",
    "# Group2TextMethod   = 'line'\n",
    "\n",
    "# Text2SentMethod  = 'whole'\n",
    "\n",
    "# Sent2TokenMethod = 'pos_en' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "# TOKENLevel = 'word'\n",
    "\n",
    "# min_token_freq = 1\n",
    "\n",
    "# use_hyper = ['pos_en']\n",
    "\n",
    "# anno = 'json_annotation'\n",
    "# anno_keywords = {\n",
    "#     'strText': 'content',\n",
    "#     'labels': 'annotation',\n",
    "# }\n",
    "\n",
    "# BasicObject.INIT(CORPUSPath, \n",
    "#                  Corpus2GroupMethod, \n",
    "#                  Group2TextMethod, \n",
    "#                  Text2SentMethod, \n",
    "#                  Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "#                  use_hyper = use_hyper, \n",
    "#                  anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPBA2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NLPBA2004/sampletest1.iob2\n",
      "corpus/NLPBA2004/Genia4ERtask1.iob2\n",
      "Total Num of All    Tokens 456790\n",
      "Total Num of Unique Tokens 20663\n",
      "CORPUS\tit is Dumped into file: data/NLPBA2004/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NLPBA2004/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/NLPBA2004/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 16746\n",
      "SENT\tit is Dumped into file: data/NLPBA2004/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16746\n",
      "TOKEN\tit is Dumped into file: data/NLPBA2004/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 456790\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/NLPBA2004/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/pos_en-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NLPBA2004/word/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/NLPBA2004/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 20663\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NLPBA2004/'\n",
    "\n",
    "Corpus2GroupMethod = '.iob2'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' '\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': '\\t',\n",
    "    'connector': ' ', \n",
    "    'suffix': False,\n",
    "    \"change_tags\": False, \n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aberrant activation of AP-1 by gp160 in CD4 positive T cells could result in up-regulation of cytokines containing AP-1 sites , e.g . interleukin-3 and granulocyte macrophage colony-stimulating factor , and concurrently lead to T cell unresponsiveness by inhibiting interleukin-2 secretion .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'O'),\n",
       " ('aberrant', 'O'),\n",
       " ('activation', 'O'),\n",
       " ('of', 'O'),\n",
       " ('AP-1', 'protein-B'),\n",
       " ('by', 'O'),\n",
       " ('gp160', 'protein-B'),\n",
       " ('in', 'O'),\n",
       " ('CD4', 'cell_type-B'),\n",
       " ('positive', 'cell_type-I'),\n",
       " ('T', 'cell_type-I'),\n",
       " ('cells', 'cell_type-I'),\n",
       " ('could', 'O'),\n",
       " ('result', 'O'),\n",
       " ('in', 'O'),\n",
       " ('up-regulation', 'O'),\n",
       " ('of', 'O'),\n",
       " ('cytokines', 'protein-B'),\n",
       " ('containing', 'O'),\n",
       " ('AP-1', 'DNA-B'),\n",
       " ('sites', 'DNA-I'),\n",
       " (',', 'O'),\n",
       " ('e.g', 'O'),\n",
       " ('.', 'O'),\n",
       " ('interleukin-3', 'protein-B'),\n",
       " ('and', 'O'),\n",
       " ('granulocyte', 'protein-B'),\n",
       " ('macrophage', 'protein-I'),\n",
       " ('colony-stimulating', 'protein-I'),\n",
       " ('factor', 'protein-I'),\n",
       " (',', 'O'),\n",
       " ('and', 'O'),\n",
       " ('concurrently', 'O'),\n",
       " ('lead', 'O'),\n",
       " ('to', 'O'),\n",
       " ('T', 'cell_type-B'),\n",
       " ('cell', 'cell_type-I'),\n",
       " ('unresponsiveness', 'O'),\n",
       " ('by', 'O'),\n",
       " ('inhibiting', 'O'),\n",
       " ('interleukin-2', 'protein-B'),\n",
       " ('secretion', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "st = Sentence(16745)\n",
    "print(st.sentence)\n",
    "\n",
    "list(zip(st.sentence.split(' '), st.get_stored_hyperstring('annoE', tagScheme= 'BIO')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'protein': 30393,\n",
       " 'cell_type': 6774,\n",
       " 'DNA': 9539,\n",
       " 'cell_line': 3832,\n",
       " 'RNA': 966}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T13:16:16.948442Z",
     "start_time": "2019-09-23T13:16:16.287972Z"
    }
   },
   "outputs": [],
   "source": [
    "# from smart_open import smart_open\n",
    "\n",
    "\n",
    "# folderPath = 'corpus/MedPos/SegDataSheet.DB'\n",
    "\n",
    "# new_path = 'corpus/MedPos/New.txt'\n",
    "# with open(new_path, 'w', encoding='utf-8') as f1:\n",
    "#     with open(folderPath, 'r', encoding='utf-8') as f:\n",
    "#         lastlineidx = 0\n",
    "#         lasttextidx = 0\n",
    "#         for line in f:\n",
    "#             # print(line)\n",
    "#             group_text_lineidx, text, label, start, end = line.replace('\\n', '').split('\\t')\n",
    "\n",
    "#             lineidx = int(group_text_lineidx.split('_')[-1])\n",
    "#             textidx = int(group_text_lineidx.split('_')[-2])\n",
    "#             # print(group_text_lineidx)\n",
    "#             # print(lineidx)\n",
    "#             # print(line, text, label, start, end)\n",
    "#             label = label.split('-')[0]\n",
    "#             for idx, char in enumerate(text):\n",
    "#                 suffix = '-B' if idx == 0 else '-I'\n",
    "#                 if lineidx != lastlineidx or textidx != lasttextidx:\n",
    "#                     lastlineidx = lineidx\n",
    "#                     lasttextidx = textidx\n",
    "#                     # print('\\n')\n",
    "#                     f1.write('\\n')\n",
    "#                 # print(char, label + suffix)\n",
    "#                 f1.write(char + ' ' + label + suffix + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.084885Z",
     "start_time": "2019-10-22T11:00:15.084526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/MedPos/New.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.653 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 326091\n",
      "Total Num of Unique Tokens 1597\n",
      "CORPUS\tit is Dumped into file: data/MedPos/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/MedPos/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/MedPos/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 7369\n",
      "SENT\tit is Dumped into file: data/MedPos/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 7369\n",
      "TOKEN\tit is Dumped into file: data/MedPos/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 326091\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/MedPos/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/MedPos/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/MedPos/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 129\n",
      "\t\tWrite to: data/MedPos/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/MedPos/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1597\n",
      "\t\tWrite to: data/MedPos/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/MedPos/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': True,\n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.097029Z",
     "start_time": "2019-10-22T11:00:33.087824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腹 部 平 坦 , 无 腹 壁 静 脉 曲 张 , 无 压 痛 、 反 跳 痛 , 未 触 及 包 块 。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 1804\n",
    "\n",
    "st = Sentence(locidx)\n",
    "\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.102182Z",
     "start_time": "2019-10-22T11:00:33.098459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['f', 'ù'],\n",
       " ['b', 'ù'],\n",
       " ['p', 'íng'],\n",
       " ['t', 'ǎn'],\n",
       " [','],\n",
       " ['w', 'ú'],\n",
       " ['f', 'ù'],\n",
       " ['b', 'ì'],\n",
       " ['j', 'ìng'],\n",
       " ['m', 'à', 'i'],\n",
       " ['q', 'ū'],\n",
       " ['zh', 'āng'],\n",
       " [','],\n",
       " ['w', 'ú'],\n",
       " ['y', 'ā'],\n",
       " ['t', 'òng'],\n",
       " ['、'],\n",
       " ['f', 'ǎn'],\n",
       " ['t', 'i', 'à', 'o'],\n",
       " ['t', 'òng'],\n",
       " [','],\n",
       " ['w', 'è', 'i'],\n",
       " ['ch', 'ù'],\n",
       " ['j', 'í'],\n",
       " ['b', 'ā', 'o'],\n",
       " ['k', 'u', 'à', 'i'],\n",
       " ['。']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_grain_str('pinyin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.107079Z",
     "start_time": "2019-10-22T11:00:33.103516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['身体部位-B'],\n",
       " ['空间概念-B'],\n",
       " ['定性-B'],\n",
       " ['定性-I'],\n",
       " ['标点-B'],\n",
       " ['无-B'],\n",
       " ['身体部位-B'],\n",
       " ['身体部位-I'],\n",
       " ['身体部位-B'],\n",
       " ['身体部位-I'],\n",
       " ['体征与症状-B'],\n",
       " ['体征与症状-I'],\n",
       " ['标点-B'],\n",
       " ['无-B'],\n",
       " ['体征与症状-B'],\n",
       " ['体征与症状-I'],\n",
       " ['连词-B'],\n",
       " ['体征与症状-B'],\n",
       " ['体征与症状-I'],\n",
       " ['体征与症状-I'],\n",
       " ['标点-B'],\n",
       " ['无-B'],\n",
       " ['有-B'],\n",
       " ['有-I'],\n",
       " ['体征与症状-B'],\n",
       " ['体征与症状-I'],\n",
       " ['标点-B']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_grain_str('annoE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.836720Z",
     "start_time": "2019-10-22T11:00:33.108355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'数字': 14014,\n",
       " '标点': 42448,\n",
       " '人群': 3059,\n",
       " '定性': 14508,\n",
       " '疾病': 3542,\n",
       " '体征与症状': 21261,\n",
       " '因果': 1359,\n",
       " '连词': 8700,\n",
       " '时间单位': 3928,\n",
       " '事件': 4442,\n",
       " '虚词': 3753,\n",
       " '时间概念': 4514,\n",
       " '无': 13642,\n",
       " '有': 8745,\n",
       " '身体物质': 1103,\n",
       " '身体部位': 19001,\n",
       " '地名机构名': 2662,\n",
       " '医疗行为': 3093,\n",
       " '空间概念': 7626,\n",
       " '检查项目': 3200,\n",
       " '或许': 388,\n",
       " '治疗项目': 1579,\n",
       " '身体功能': 5179,\n",
       " '临床属性': 6003,\n",
       " '数学符号': 1098,\n",
       " '单位': 3763,\n",
       " '药物': 728,\n",
       " '代词': 477,\n",
       " '医疗仪器': 432,\n",
       " '物体': 883,\n",
       " '生物': 150,\n",
       " 'notsure': 14}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T11:00:33.840467Z",
     "start_time": "2019-10-22T11:00:33.837881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:51:29.486288Z",
     "start_time": "2019-10-24T02:51:29.480865Z"
    }
   },
   "outputs": [],
   "source": [
    "# [i for i in d]\n",
    "\n",
    "\n",
    "# a = [\n",
    "#     '生物',\n",
    "#     '物体',\n",
    "#     '地名机构名',\n",
    "#     '人群',\n",
    "#     '事件',\n",
    "    \n",
    "#     '身体部位',\n",
    "#     '身体物质',\n",
    "#     '临床属性',\n",
    "#     '身体功能',\n",
    "    \n",
    "    \n",
    "#     '医疗行为',\n",
    "#     '医疗仪器',\n",
    "#     '治疗项目',\n",
    "#     '检查项目',\n",
    "#     '药物',\n",
    "    \n",
    "    \n",
    "    \n",
    "#     '时间概念',\n",
    "#     '时间单位',\n",
    "     \n",
    "#     '空间概念',\n",
    "#     '数学符号',\n",
    "#     '数字',\n",
    "#     '单位',\n",
    " \n",
    "#     '体征与症状',\n",
    "#     '疾病',\n",
    "    \n",
    "    \n",
    "#     '无',\n",
    "#     '有',\n",
    "#     '或许',\n",
    "#     '定性',\n",
    "    \n",
    "#     '标点',\n",
    "#     '代词',\n",
    "#      '连词',\n",
    "#      '虚词',\n",
    "#     '因果',\n",
    "\n",
    "\n",
    " \n",
    "#  'notsure']\n",
    "\n",
    "# a\n",
    "\n",
    "# new_dict = [{'Tag': i, 'Num': d[i]} for i in a]\n",
    "# new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHIP2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T03:02:21.454746Z",
     "start_time": "2019-10-24T03:02:21.304176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt; \n",
      "&gt;\n",
      "&gt; \n",
      "&lt; \n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt; \n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt;\n",
      "&gt;\n",
      "&lt;\n",
      "&lt;\n",
      "&gt;\n",
      "&gt; \n",
      "&gt;\n"
     ]
    }
   ],
   "source": [
    "from smart_open import smart_open\n",
    "\n",
    "\n",
    "folderPath = 'corpus/CHIP2019/train/train_data.txt'\n",
    "\n",
    "# new_path = 'corpus/MedPos/New.txt'\n",
    "All_Data = {}\n",
    "with open(folderPath, 'r', encoding='utf-8') as f:\n",
    "\n",
    "    for line in f:\n",
    "        # print(line)\n",
    "        try:\n",
    "            idx, category, sentence = line.replace('\\n', '').split('\\t')\n",
    "        except:\n",
    "            print(line)\n",
    "            continue\n",
    "        # print(category)\n",
    "        # print(sentence)\n",
    "        for bad_token in ['&gt; ', '&lt; ', '&gt;', '&lt;']:\n",
    "            if bad_token in sentence:\n",
    "                sentence = sentence.replace(bad_token, '')\n",
    "                print(bad_token)\n",
    "                \n",
    "        if category in All_Data:\n",
    "            All_Data[category].append(sentence)\n",
    "        else:\n",
    "            All_Data[category] = [sentence]\n",
    "                \n",
    "        \n",
    "import pickle\n",
    "\n",
    "# for category, data in All_Data.items():\n",
    "#     with open('clsdata/validation/' + category + '.p', 'wb') as handle:\n",
    "#         pickle.dump(data, handle)\n",
    "        \n",
    "\n",
    "for category, data in All_Data.items():\n",
    "    with open('clsdata/train/' + category + '.txt', 'w') as f:\n",
    "        f.write('\\n'.join(data))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T07:41:26.274526Z",
     "start_time": "2019-10-24T07:41:26.269378Z"
    }
   },
   "outputs": [],
   "source": [
    "# from smart_open import smart_open\n",
    "\n",
    "\n",
    "# folderPath = 'corpus/CHIP2019/validation/validation_data.txt'\n",
    "\n",
    "# # new_path = 'corpus/MedPos/New.txt'\n",
    "# All_Data = {}\n",
    "# with open(folderPath, 'r', encoding='utf-8') as f:\n",
    "\n",
    "#     for line in f:\n",
    "#         # print(line)\n",
    "#         try:\n",
    "#             idx, category, sentence = line.replace('\\n', '').split('\\t')\n",
    "#         except:\n",
    "#             print(line)\n",
    "#             continue\n",
    "#         # print(category)\n",
    "#         # print(sentence)\n",
    "#         for bad_token in ['&gt; ', '&lt; ', '&gt;', '&lt;']:\n",
    "#             if bad_token in sentence:\n",
    "#                 sentence = sentence.replace(bad_token, '')\n",
    "#                 print(bad_token)\n",
    "                \n",
    "#         if category in All_Data:\n",
    "#             All_Data[category].append(sentence)\n",
    "#         else:\n",
    "#             All_Data[category] = [sentence]\n",
    "                \n",
    "        \n",
    "# import pickle\n",
    "\n",
    "# # for category, data in All_Data.items():\n",
    "# #     with open('clsdata/validation/' + category + '.p', 'wb') as handle:\n",
    "# #         pickle.dump(data, handle)\n",
    "        \n",
    "\n",
    "# for category, data in All_Data.items():\n",
    "#     with open('clsdata/validation/' + category + '.txt', 'w') as f:\n",
    "#         f.write('\\n'.join(data))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T07:42:06.642830Z",
     "start_time": "2019-10-24T07:41:43.209553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/CHIP2019_new/train/Address.txt\n",
      "corpus/CHIP2019_new/train/Addictive Behavior.txt\n",
      "corpus/CHIP2019_new/train/Compliance with Protocol.txt\n",
      "corpus/CHIP2019_new/train/Symptom.txt\n",
      "corpus/CHIP2019_new/train/Blood Donation.txt\n",
      "corpus/CHIP2019_new/train/Non-Neoplasm Disease Stage.txt\n",
      "corpus/CHIP2019_new/train/Gender.txt\n",
      "corpus/CHIP2019_new/train/Sign.txt\n",
      "corpus/CHIP2019_new/train/Oral related.txt\n",
      "corpus/CHIP2019_new/train/Disease.txt\n",
      "corpus/CHIP2019_new/train/Smoking Status.txt\n",
      "corpus/CHIP2019_new/train/Researcher Decision.txt\n",
      "corpus/CHIP2019_new/train/Education.txt\n",
      "corpus/CHIP2019_new/train/Neoplasm Status.txt\n",
      "corpus/CHIP2019_new/train/Device.txt\n",
      "corpus/CHIP2019_new/train/Multiple.txt\n",
      "corpus/CHIP2019_new/train/Enrollment in other studies.txt\n",
      "corpus/CHIP2019_new/train/Therapy or Surgery.txt\n",
      "corpus/CHIP2019_new/train/Disabilities.txt\n",
      "corpus/CHIP2019_new/train/Life Expectancy.txt\n",
      "corpus/CHIP2019_new/train/Diagnostic.txt\n",
      "corpus/CHIP2019_new/train/Sexual related.txt\n",
      "corpus/CHIP2019_new/train/Special Patient Characteristic.txt\n",
      "corpus/CHIP2019_new/train/Encounter.txt\n",
      "corpus/CHIP2019_new/train/Bedtime.txt\n",
      "corpus/CHIP2019_new/train/Consent.txt\n",
      "corpus/CHIP2019_new/train/Alcohol Consumer.txt\n",
      "corpus/CHIP2019_new/train/Healthy.txt\n",
      "corpus/CHIP2019_new/train/Organ or Tissue Status.txt\n",
      "corpus/CHIP2019_new/train/Pharmaceutical Substance or Drug.txt\n",
      "corpus/CHIP2019_new/train/Age.txt\n",
      "corpus/CHIP2019_new/train/Laboratory Examinations.txt\n",
      "corpus/CHIP2019_new/train/Allergy Intolerance.txt\n",
      "corpus/CHIP2019_new/train/Ethical Audit.txt\n",
      "corpus/CHIP2019_new/train/Literacy.txt\n",
      "corpus/CHIP2019_new/train/Receptor Status.txt\n",
      "corpus/CHIP2019_new/train/Risk Assessment.txt\n",
      "corpus/CHIP2019_new/train/Capacity.txt\n",
      "corpus/CHIP2019_new/train/Exercise.txt\n",
      "corpus/CHIP2019_new/train/Diet.txt\n",
      "corpus/CHIP2019_new/train/Ethnicity.txt\n",
      "corpus/CHIP2019_new/train/Pregnancy-related Activity.txt\n",
      "corpus/CHIP2019_new/train/Data Accessible.txt\n",
      "corpus/CHIP2019_new/train/Nursing.txt\n",
      "corpus/CHIP2019_new/validation/Address.txt\n",
      "corpus/CHIP2019_new/validation/Addictive Behavior.txt\n",
      "corpus/CHIP2019_new/validation/Compliance with Protocol.txt\n",
      "corpus/CHIP2019_new/validation/Symptom.txt\n",
      "corpus/CHIP2019_new/validation/Blood Donation.txt\n",
      "corpus/CHIP2019_new/validation/Non-Neoplasm Disease Stage.txt\n",
      "corpus/CHIP2019_new/validation/Gender.txt\n",
      "corpus/CHIP2019_new/validation/Sign.txt\n",
      "corpus/CHIP2019_new/validation/Oral related.txt\n",
      "corpus/CHIP2019_new/validation/Disease.txt\n",
      "corpus/CHIP2019_new/validation/Smoking Status.txt\n",
      "corpus/CHIP2019_new/validation/Researcher Decision.txt\n",
      "corpus/CHIP2019_new/validation/Education.txt\n",
      "corpus/CHIP2019_new/validation/Neoplasm Status.txt\n",
      "corpus/CHIP2019_new/validation/Device.txt\n",
      "corpus/CHIP2019_new/validation/Multiple.txt\n",
      "corpus/CHIP2019_new/validation/Enrollment in other studies.txt\n",
      "corpus/CHIP2019_new/validation/Therapy or Surgery.txt\n",
      "corpus/CHIP2019_new/validation/Disabilities.txt\n",
      "corpus/CHIP2019_new/validation/Life Expectancy.txt\n",
      "corpus/CHIP2019_new/validation/Diagnostic.txt\n",
      "corpus/CHIP2019_new/validation/Sexual related.txt\n",
      "corpus/CHIP2019_new/validation/Special Patient Characteristic.txt\n",
      "corpus/CHIP2019_new/validation/Encounter.txt\n",
      "corpus/CHIP2019_new/validation/Bedtime.txt\n",
      "corpus/CHIP2019_new/validation/Consent.txt\n",
      "corpus/CHIP2019_new/validation/Alcohol Consumer.txt\n",
      "corpus/CHIP2019_new/validation/Healthy.txt\n",
      "corpus/CHIP2019_new/validation/Organ or Tissue Status.txt\n",
      "corpus/CHIP2019_new/validation/Pharmaceutical Substance or Drug.txt\n",
      "corpus/CHIP2019_new/validation/Age.txt\n",
      "corpus/CHIP2019_new/validation/Laboratory Examinations.txt\n",
      "corpus/CHIP2019_new/validation/Allergy Intolerance.txt\n",
      "corpus/CHIP2019_new/validation/Ethical Audit.txt\n",
      "corpus/CHIP2019_new/validation/Literacy.txt\n",
      "corpus/CHIP2019_new/validation/Receptor Status.txt\n",
      "corpus/CHIP2019_new/validation/Risk Assessment.txt\n",
      "corpus/CHIP2019_new/validation/Capacity.txt\n",
      "corpus/CHIP2019_new/validation/Exercise.txt\n",
      "corpus/CHIP2019_new/validation/Diet.txt\n",
      "corpus/CHIP2019_new/validation/Ethnicity.txt\n",
      "corpus/CHIP2019_new/validation/Pregnancy-related Activity.txt\n",
      "corpus/CHIP2019_new/validation/Data Accessible.txt\n",
      "corpus/CHIP2019_new/validation/Nursing.txt\n",
      "Total Num of All    Tokens 786482\n",
      "Total Num of Unique Tokens 2524\n",
      "CORPUS\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 88\n",
      "TEXT\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 30584\n",
      "SENT\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 30584\n",
      "TOKEN\tit is Dumped into file: data/CHIP2019_new/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 786482\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/CHIP2019_new/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/CHIP2019_new/char/Vocab/pos-bioes.tsv\n",
      "token   \tis Dumped into file: data/CHIP2019_new/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2524\n",
      "\t\tWrite to: data/CHIP2019_new/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/CHIP2019_new/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = False\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = False, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCKS2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:46:03.419303Z",
     "start_time": "2019-10-16T06:45:50.264010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/CCKS2017/病史特点\n",
      "corpus/CCKS2017/出院情况\n",
      "corpus/CCKS2017/一般项目\n",
      "corpus/CCKS2017/诊疗经过\n",
      "Total Num of All    Tokens 260781\n",
      "Total Num of Unique Tokens 1731\n",
      "CORPUS\tit is Dumped into file: data/CCKS2017/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/CCKS2017/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 4\n",
      "TEXT\tit is Dumped into file: data/CCKS2017/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1198\n",
      "SENT\tit is Dumped into file: data/CCKS2017/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 8644\n",
      "TOKEN\tit is Dumped into file: data/CCKS2017/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 260781\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/CCKS2017/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/CCKS2017/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/CCKS2017/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/CCKS2017/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/CCKS2017/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1731\n",
      "\t\tWrite to: data/CCKS2017/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/CCKS2017/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.NER',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, # if 0, indexed from zero\n",
    "    'notRightOpen' : 1, # if 0, Right is Open, is 1 not Open\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:47:12.363856Z",
     "start_time": "2019-10-16T06:47:12.358482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'气 管 居 中 , 甲 状 腺 无 肿 大 。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 100\n",
    "\n",
    "st = Sentence(locidx)\n",
    "\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:47:26.230559Z",
     "start_time": "2019-10-16T06:47:26.221984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['身体部位-B',\n",
       " '身体部位-E',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " '身体部位-B',\n",
       " '身体部位-I',\n",
       " '身体部位-E',\n",
       " 'O',\n",
       " '症状和体征-B',\n",
       " '症状和体征-E',\n",
       " 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:47:35.744475Z",
     "start_time": "2019-10-16T06:47:35.145054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'治疗': 1048, '身体部位': 10719, '症状和体征': 7830, '疾病和诊断': 722, '检查和检验': 9546}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T06:52:12.993792Z",
     "start_time": "2019-10-16T06:51:57.523457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/LuohuNER/Digestion\n",
      "corpus/LuohuNER/Cardiovascular\n",
      "corpus/LuohuNER/Urinary\n",
      "corpus/LuohuNER/Respiratory\n",
      "corpus/LuohuNER/Gynecology\n",
      "Total Num of All    Tokens 324362\n",
      "Total Num of Unique Tokens 1596\n",
      "CORPUS\tit is Dumped into file: data/LuohuNER/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuNER/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/LuohuNER/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 500\n",
      "SENT\tit is Dumped into file: data/LuohuNER/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 8663\n",
      "TOKEN\tit is Dumped into file: data/LuohuNER/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 324362\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuNER/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuNER/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/LuohuNER/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/LuohuNER/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuNER/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1596\n",
      "\t\tWrite to: data/LuohuNER/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/LuohuNER/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.NER',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, # if 0, indexed from zero\n",
    "    'notRightOpen' : 0, # if 0, Right is Open, is 1 not Open\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luohu NER 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T12:42:36.999801Z",
     "start_time": "2019-10-22T12:42:11.936559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/LuohuNER750Neat/呼吸内科\n",
      "corpus/LuohuNER750Neat/消化内科\n",
      "corpus/LuohuNER750Neat/妇一科\n",
      "corpus/LuohuNER750Neat/泌尿外科\n",
      "corpus/LuohuNER750Neat/心血管内科\n",
      "Total Num of All    Tokens 598039\n",
      "Total Num of Unique Tokens 1966\n",
      "CORPUS\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 5\n",
      "TEXT\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 749\n",
      "SENT\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16479\n",
      "TOKEN\tit is Dumped into file: data/LuohuNER750Neat/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 598039\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/LuohuNER750Neat/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/LuohuNER750Neat/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/LuohuNER750Neat/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/LuohuNER750Neat/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/LuohuNER750Neat/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1966\n",
      "\t\tWrite to: data/LuohuNER750Neat/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/LuohuNER750Neat/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.NER',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, # if 0, indexed from zero\n",
    "    'notRightOpen' : 0, # if 0, Right is Open, is 1 not Open\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T12:42:38.177913Z",
     "start_time": "2019-10-22T12:42:37.002244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'异常': 39465, '异常分类': 7863, '主体': 28079, '检查': 25404, '治疗': 7726}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Handle Labels\n",
    "\n",
    "## strText and SSET\n",
    "\n",
    "For each text, we must get a strText and SSET, which meet the following restriction.\n",
    "\n",
    "Besides, we also need a strSents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结肠多发息肉。\n",
      "患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('多发息肉', 2, 6, '疾病'),\n",
       " ('慢性', 15, 17, '修饰'),\n",
       " ('多发息肉', 29, 33, '疾病'),\n",
       " ('3月余', 33, 36, '修饰'),\n",
       " ('无阳性体征', 43, 48, '不确定')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "print(strText)\n",
    "\n",
    "strAnnoText = '''标注文本名称:/Users/zhangling/Documents/新标的数据530/529李选-已检查/Entity/patient4378.txt\\n标注文本字数统计:87\\n多发息肉\\t3\\t6\\t疾病\\n慢性\\t16\\t17\\t修饰\\n多发息肉\\t30\\t33\\t疾病\\n3月余\\t34\\t36\\t修饰\\n无阳性体征\\t44\\t48\\t不确定\\n'''\n",
    "\n",
    "# BIOES\n",
    "\n",
    "sep = '\\t'\n",
    "SSETText = [sset.split('\\t') for sset in strAnnoText.split('\\n') if sep in sset]\n",
    "\n",
    "\n",
    "notZeroIndex = 1 \n",
    "for idx, sset in enumerate(SSETText):\n",
    "    data = (sset[0], int(sset[1]) - notZeroIndex, int(sset[2]), sset[-1])\n",
    "    SSETText[idx] = data\n",
    "\n",
    "\n",
    "# check\n",
    "for sset in SSETText:\n",
    "    try:\n",
    "        assert strText[sset[1]: sset[2]] == sset[0]\n",
    "    except:\n",
    "        print('strText:', strText[sset[1] : sset[2]])\n",
    "        print('SSETText:', sset[0])\n",
    "        \n",
    "SSETText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCITText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['结', 0, 'O'],\n",
       " ['肠', 1, 'O'],\n",
       " ['多', 2, '疾病-B'],\n",
       " ['发', 3, '疾病-I'],\n",
       " ['息', 4, '疾病-I'],\n",
       " ['肉', 5, '疾病-E'],\n",
       " ['。', 6, 'O'],\n",
       " ['\\n', 7, 'O'],\n",
       " ['患', 8, 'O'],\n",
       " ['中', 9, 'O'],\n",
       " ['老', 10, 'O'],\n",
       " ['年', 11, 'O'],\n",
       " ['男', 12, 'O'],\n",
       " ['性', 13, 'O'],\n",
       " [',', 14, 'O'],\n",
       " ['慢', 15, '修饰-B'],\n",
       " ['性', 16, '修饰-E'],\n",
       " ['病', 17, 'O'],\n",
       " ['程', 18, 'O'],\n",
       " ['。', 19, 'O'],\n",
       " [' ', 20, 'O'],\n",
       " ['因', 21, 'O'],\n",
       " ['“', 22, 'O'],\n",
       " ['体', 23, 'O'],\n",
       " ['检', 24, 'O'],\n",
       " ['发', 25, 'O'],\n",
       " ['现', 26, 'O'],\n",
       " ['大', 27, 'O'],\n",
       " ['肠', 28, 'O'],\n",
       " ['多', 29, '疾病-B'],\n",
       " ['发', 30, '疾病-I'],\n",
       " ['息', 31, '疾病-I'],\n",
       " ['肉', 32, '疾病-E'],\n",
       " ['3', 33, '修饰-B'],\n",
       " ['月', 34, '修饰-I'],\n",
       " ['余', 35, '修饰-E'],\n",
       " ['”', 36, 'O'],\n",
       " ['入', 37, 'O'],\n",
       " ['院', 38, 'O'],\n",
       " ['。', 39, 'O'],\n",
       " ['查', 40, 'O'],\n",
       " ['体', 41, 'O'],\n",
       " [':', 42, 'O'],\n",
       " ['无', 43, '不确定-B'],\n",
       " ['阳', 44, '不确定-I'],\n",
       " ['性', 45, '不确定-I'],\n",
       " ['体', 46, '不确定-I'],\n",
       " ['征', 47, '不确定-E'],\n",
       " ['。', 48, 'O']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "\n",
    "SSETText = [ ('多发息肉', 2, 6, '疾病'),\n",
    "             ('慢性', 15, 17, '修饰'),\n",
    "             ('多发息肉', 29, 33, '疾病'),\n",
    "             ('3月余', 33, 36, '修饰'),\n",
    "             ('无阳性体征', 43, 48, '不确定')]\n",
    "\n",
    "def getCITText(strText, SSETText, TOKENLevel='char'):\n",
    "    len(SSETText) > 0 \n",
    "    if TOKENLevel == 'char':\n",
    "        for sset in SSETText:\n",
    "            try:\n",
    "                assert strText[sset[1]: sset[2]] == sset[0]\n",
    "            except:\n",
    "                print('strText:', strText[sset[1] : sset[2]])\n",
    "                print('SSETText:', sset[0])\n",
    "        CITAnnoText = []\n",
    "        for sset in SSETText:\n",
    "            # BIOES\n",
    "            strAnno, s, e, tag = sset\n",
    "            CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "            CIT[-1][2] = tag + '-E'\n",
    "            CIT[ 0][2] = tag + '-B'\n",
    "            if len(CIT) == 1:\n",
    "                CIT[0][2] = tag + '-S' \n",
    "            CITAnnoText.extend(CIT)\n",
    "\n",
    "        # print(strAnnoText)\n",
    "        CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "        for citAnno in CITAnnoText:\n",
    "            c, idx, t = citAnno\n",
    "            assert CITText[idx][0] == c\n",
    "            CITText[idx] = citAnno\n",
    "\n",
    "    elif TOKENLevel == 'word':\n",
    "        CITText = []\n",
    "        for idx, sset in enumerate(SSETText):\n",
    "            try:\n",
    "                assert sset[0] == strText[idx]\n",
    "            except:\n",
    "                print(strText)[idx]\n",
    "                print(sset[0])\n",
    "\n",
    "            CITText.append(sset)\n",
    "    return CITText\n",
    "\n",
    "\n",
    "CITText = getCITText(strText, SSETText, TOKENLevel='char')\n",
    "CITText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCITSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['结', 0, 'O'],\n",
       "  ['肠', 1, 'O'],\n",
       "  ['多', 2, '疾病-B'],\n",
       "  ['发', 3, '疾病-I'],\n",
       "  ['息', 4, '疾病-I'],\n",
       "  ['肉', 5, '疾病-E'],\n",
       "  ['。', 6, 'O']],\n",
       " [['患', 0, 'O'],\n",
       "  ['中', 1, 'O'],\n",
       "  ['老', 2, 'O'],\n",
       "  ['年', 3, 'O'],\n",
       "  ['男', 4, 'O'],\n",
       "  ['性', 5, 'O'],\n",
       "  [',', 6, 'O'],\n",
       "  ['慢', 7, '修饰-B'],\n",
       "  ['性', 8, '修饰-E'],\n",
       "  ['病', 9, 'O'],\n",
       "  ['程', 10, 'O'],\n",
       "  ['。', 11, 'O']],\n",
       " [['因', 0, 'O'],\n",
       "  ['“', 1, 'O'],\n",
       "  ['体', 2, 'O'],\n",
       "  ['检', 3, 'O'],\n",
       "  ['发', 4, 'O'],\n",
       "  ['现', 5, 'O'],\n",
       "  ['大', 6, 'O'],\n",
       "  ['肠', 7, 'O'],\n",
       "  ['多', 8, '疾病-B'],\n",
       "  ['发', 9, '疾病-I'],\n",
       "  ['息', 10, '疾病-I'],\n",
       "  ['肉', 11, '疾病-E'],\n",
       "  ['3', 12, '修饰-B'],\n",
       "  ['月', 13, '修饰-I'],\n",
       "  ['余', 14, '修饰-E'],\n",
       "  ['”', 15, 'O'],\n",
       "  ['入', 16, 'O'],\n",
       "  ['院', 17, 'O'],\n",
       "  ['。', 18, 'O']],\n",
       " [['查', 0, 'O'],\n",
       "  ['体', 1, 'O'],\n",
       "  [':', 2, 'O'],\n",
       "  ['无', 3, '不确定-B'],\n",
       "  ['阳', 4, '不确定-I'],\n",
       "  ['性', 5, '不确定-I'],\n",
       "  ['体', 6, '不确定-I'],\n",
       "  ['征', 7, '不确定-E'],\n",
       "  ['。', 8, 'O']]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strSents = ['结肠多发息肉。', '患中老年男性,慢性病程。', '因“体检发现大肠多发息肉3月余”入院。', '查体:无阳性体征。']\n",
    "\n",
    "CITText  =  [['结', 0, 'O'],\n",
    "             ['肠', 1, 'O'],\n",
    "             ['多', 2, '疾病-B'],\n",
    "             ['发', 3, '疾病-I'],\n",
    "             ['息', 4, '疾病-I'],\n",
    "             ['肉', 5, '疾病-E'],\n",
    "             ['。', 6, 'O'],\n",
    "             ['\\n', 7, 'O'],\n",
    "             ['患', 8, 'O'],\n",
    "             ['中', 9, 'O'],\n",
    "             ['老', 10, 'O'],\n",
    "             ['年', 11, 'O'],\n",
    "             ['男', 12, 'O'],\n",
    "             ['性', 13, 'O'],\n",
    "             [',', 14, 'O'],\n",
    "             ['慢', 15, '修饰-B'],\n",
    "             ['性', 16, '修饰-E'],\n",
    "             ['病', 17, 'O'],\n",
    "             ['程', 18, 'O'],\n",
    "             ['。', 19, 'O'],\n",
    "             [' ', 20, 'O'],\n",
    "             ['因', 21, 'O'],\n",
    "             ['“', 22, 'O'],\n",
    "             ['体', 23, 'O'],\n",
    "             ['检', 24, 'O'],\n",
    "             ['发', 25, 'O'],\n",
    "             ['现', 26, 'O'],\n",
    "             ['大', 27, 'O'],\n",
    "             ['肠', 28, 'O'],\n",
    "             ['多', 29, '疾病-B'],\n",
    "             ['发', 30, '疾病-I'],\n",
    "             ['息', 31, '疾病-I'],\n",
    "             ['肉', 32, '疾病-E'],\n",
    "             ['3', 33, '修饰-B'],\n",
    "             ['月', 34, '修饰-I'],\n",
    "             ['余', 35, '修饰-E'],\n",
    "             ['”', 36, 'O'],\n",
    "             ['入', 37, 'O'],\n",
    "             ['院', 38, 'O'],\n",
    "             ['。', 39, 'O'],\n",
    "             ['查', 40, 'O'],\n",
    "             ['体', 41, 'O'],\n",
    "             [':', 42, 'O'],\n",
    "             ['无', 43, '不确定-B'],\n",
    "             ['阳', 44, '不确定-I'],\n",
    "             ['性', 45, '不确定-I'],\n",
    "             ['体', 46, '不确定-I'],\n",
    "             ['征', 47, '不确定-E'],\n",
    "             ['。', 48, 'O']]\n",
    "\n",
    "def getCITSents(strSents, CITText):\n",
    "    lenLastSent = 0\n",
    "    collapse    = 0 # don't need to move \n",
    "    CITSents = []\n",
    "    for strSent in strSents:\n",
    "        CITSent = []\n",
    "        for sentTokenIdx, c in enumerate(strSent):\n",
    "            # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "            txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "            cT, _, tT = CITText[txtTokenIdx]\n",
    "            while c != cT and c != ' ':\n",
    "                collapse = collapse + 1\n",
    "                txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "                cT, _, tT = CITText[txtTokenIdx]\n",
    "            CITSent.append([c,sentTokenIdx, tT])\n",
    "        lenLastSent = lenLastSent + len(strSent)\n",
    "        CITSents.append(CITSent)\n",
    "    # CITSents\n",
    "    # Here we get CITSents  \n",
    "    return CITSents\n",
    "       \n",
    "    \n",
    "CITSents = getCITSents(strSents, CITText)\n",
    "CITSents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getSSET_from_CIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['结', 0, 'O'], ['肠', 1, 'O'], ['多', 2, '疾病-B'], ['发', 3, '疾病-I'], ['息', 4, '疾病-I'], ['肉', 5, '疾病-E'], ['。', 6, 'O']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O', 'O', '疾病-B', '疾病-I', '疾病-I', '疾病-E', 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CITSent = CITSents[0]\n",
    "print(CITSent)\n",
    "tag_seq = [i[-1] for i in CITSent]\n",
    "tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSET_from_CIT(orig_seq = None, tag_seq = None, CIT = None, tag_seq_tagScheme = 'BIO', join_char = ''):\n",
    "    # orig_seq is sentence without start or end\n",
    "    # tag_seq may have start or end\n",
    "        \n",
    "    tagScheme = tag_seq_tagScheme\n",
    "    if tagScheme == 'BIOES':\n",
    "        tag_seq = [i.replace('-S', '-B').replace('-E', '-I') for i in tag_seq]\n",
    "    elif tagScheme == 'BIOE':\n",
    "        tag_seq = [i.replace('-E', '-I') for i in tag_seq]\n",
    "    elif tagScheme == 'BIOS':\n",
    "        tag_seq = [i.replace('-S', '-B') for i in tag_seq]\n",
    "    elif tagScheme == 'BIO':\n",
    "        pass\n",
    "    else:\n",
    "        print('The tagScheme', tagScheme, 'is not supported yet...')\n",
    "    \n",
    "    if not CIT:\n",
    "        # use BIO tagScheme\n",
    "        CIT = list(zip(orig_seq, range(len(orig_seq)), tag_seq))\n",
    "    taggedCIT = [cit for cit in CIT if cit[2]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedCIT)) if taggedCIT[idx][2][-2:] == '-B']\n",
    "    startIdx.append(len(taggedCIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedCIT[startIdx[i]: startIdx[i+1]]\n",
    "        string = join_char.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][1], entityAtom[-1][1] + 1\n",
    "        tag = entityAtom[0][2].split('-')[0]\n",
    "        entitiesList.append((string, start, end, tag))\n",
    "    return entitiesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get strText, strSents, and SSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
