{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical NER Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/clinical_ner_sample/MEntity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.610 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 659\n",
      "Total Num of Unique Tokens 245\n",
      "CORPUS\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 3\n",
      "SENT\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 21\n",
      "TOKEN\tit is Dumped into file: data/clinical_ner_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 659\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/clinical_ner_sample/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/clinical_ner_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 245\n",
      "\t\tWrite to: data/clinical_ner_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/clinical_ner_sample/'\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = {'pos'}\n",
    "\n",
    "anno = 'annofile4text'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.Entity',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 1, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedPOS Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dir'\n",
      "corpus/medical_pos_sample/batch2\n",
      "4\n",
      "patient5218-sent4.UMLSTag\n",
      "corpus/medical_pos_sample/batch1\n",
      "2\n",
      "patient5175-sent2.UMLSTag\n",
      "10\n",
      "patient4857-sent10.UMLSTag\n",
      "Total Num of All    Tokens 6460\n",
      "Total Num of Unique Tokens 713\n",
      "CORPUS\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 20\n",
      "SENT\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 179\n",
      "TOKEN\tit is Dumped into file: data/medical_pos_sample/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 6460\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/medical_pos_sample/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 153\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/medical_pos_sample/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 713\n",
      "\t\tWrite to: data/medical_pos_sample/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/medical_pos_sample/' # TODO\n",
    "\n",
    "Corpus2GroupMethod = 'Dir' # TODO\n",
    "\n",
    "Group2TextMethod   = 'file'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'annofile4sent'\n",
    "anno_keywords = {\n",
    "    'ANNOIden': '.UMLSTag',\n",
    "    'anno_sep' : '\\t', \n",
    "    'notZeroIndex' : 0, \n",
    "    'notRightOpen' : 0,\n",
    "}\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/boson/bosonNER.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.589 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 533491\n",
      "Total Num of Unique Tokens 3825\n",
      "CORPUS\tit is Dumped into file: data/boson/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/boson/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/boson/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tit is Dumped into file: data/boson/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10214\n",
      "TOKEN\tit is Dumped into file: data/boson/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 533491\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/boson/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/boson/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/boson/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 25\n",
      "\t\tWrite to: data/boson/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/boson/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3825\n",
      "\t\tWrite to: data/boson/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'anno_embed_in_text'\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'记 者 从 杭 州 江 干 区 公 安 分 局 了 解 到 , 经 过 一 个 多 月 的 侦 查 工 作 , 江 干 区 禁 毒 专 案 组 抓 获 吸 贩 毒 人 员 5 名 , 缴 获 “ 冰 毒 ” 4 0 0 余 克 , 毒 资 3 0 0 0 0 余 元 , 扣 押 汽 车 一 辆 。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "locidx = 2\n",
    "\n",
    "st = Sentence(locidx)\n",
    "\n",
    "st.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'org_name-B',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-E',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'org_name-B',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-I',\n",
       " 'org_name-E',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product_name': 4109,\n",
       " 'time': 4243,\n",
       " 'person_name': 5123,\n",
       " 'org_name': 2683,\n",
       " 'location': 4593,\n",
       " 'company_name': 2368}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/ResumeCN/test.char.bmes\n",
      "corpus/ResumeCN/train.char.bmes\n",
      "corpus/ResumeCN/dev.char.bmes\n",
      "Total Num of All    Tokens 149123\n",
      "Total Num of Unique Tokens 1865\n",
      "CORPUS\tit is Dumped into file: data/ResumeCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/ResumeCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/ResumeCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4476\n",
      "SENT\tit is Dumped into file: data/ResumeCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4476\n",
      "TOKEN\tit is Dumped into file: data/ResumeCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 149123\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/ResumeCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 33\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/ResumeCN/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 1865\n",
      "\t\tWrite to: data/ResumeCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/ResumeCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.bmes'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NAME': 1048,\n",
       " 'PRO': 337,\n",
       " 'EDU': 1071,\n",
       " 'TITLE': 7762,\n",
       " 'ORG': 5684,\n",
       " 'CONT': 320,\n",
       " 'RACE': 143,\n",
       " 'LOC': 55}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NewsCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NewsCN/demo.dev.char\n",
      "corpus/NewsCN/demo.test.char\n",
      "corpus/NewsCN/demo.train.char\n",
      "Total Num of All    Tokens 54635\n",
      "Total Num of Unique Tokens 2329\n",
      "CORPUS\tit is Dumped into file: data/NewsCN/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NewsCN/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/NewsCN/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1129\n",
      "SENT\tit is Dumped into file: data/NewsCN/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 1129\n",
      "TOKEN\tit is Dumped into file: data/NewsCN/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 54635\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/NewsCN/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NewsCN/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/NewsCN/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 2329\n",
      "\t\tWrite to: data/NewsCN/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NewsCN/'\n",
    "\n",
    "Corpus2GroupMethod = '.char'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = 'iter' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ',\n",
    "    'connector': '',\n",
    "    'suffix': False,\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'GPE': 1488, 'ORG': 626, 'PER': 698, 'LOC': 210}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/CoNLL-2003/eng.train.openNLP\n",
      "corpus/CoNLL-2003/eng.testb.openNLP\n",
      "corpus/CoNLL-2003/eng.testa.openNLP\n",
      "Total Num of All    Tokens 254708\n",
      "Total Num of Unique Tokens 27270\n",
      "CORPUS\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 16477\n",
      "SENT\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16477\n",
      "TOKEN\tit is Dumped into file: data/CoNLL-2003/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 254708\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/CoNLL-2003/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/pos_en-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/CoNLL-2003/word/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 17\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/CoNLL-2003/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 27270\n",
      "\t\tWrite to: data/CoNLL-2003/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/CoNLL-2003/'\n",
    "\n",
    "Corpus2GroupMethod = '.openNLP'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' ' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': ' ', # the seperation\n",
    "    'connector': ' ', \n",
    "    'suffix': False,\n",
    "    'change_tags': True, # change I-B tags\n",
    "}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares on the Dhaka Stock Exchange ( DSE ) may remain steady as small investors are expected to target mainly blue chips while overseas investors will prefer to keep to the sidelines when the market reopens after Moslem Friday weekend , brokers said .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Shares', 'O'),\n",
       " ('on', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Dhaka', 'ORG-B'),\n",
       " ('Stock', 'ORG-I'),\n",
       " ('Exchange', 'ORG-I'),\n",
       " ('(', 'O'),\n",
       " ('DSE', 'ORG-B'),\n",
       " (')', 'O'),\n",
       " ('may', 'O'),\n",
       " ('remain', 'O'),\n",
       " ('steady', 'O'),\n",
       " ('as', 'O'),\n",
       " ('small', 'O'),\n",
       " ('investors', 'O'),\n",
       " ('are', 'O'),\n",
       " ('expected', 'O'),\n",
       " ('to', 'O'),\n",
       " ('target', 'O'),\n",
       " ('mainly', 'O'),\n",
       " ('blue', 'O'),\n",
       " ('chips', 'O'),\n",
       " ('while', 'O'),\n",
       " ('overseas', 'O'),\n",
       " ('investors', 'O'),\n",
       " ('will', 'O'),\n",
       " ('prefer', 'O'),\n",
       " ('to', 'O'),\n",
       " ('keep', 'O'),\n",
       " ('to', 'O'),\n",
       " ('the', 'O'),\n",
       " ('sidelines', 'O'),\n",
       " ('when', 'O'),\n",
       " ('the', 'O'),\n",
       " ('market', 'O'),\n",
       " ('reopens', 'O'),\n",
       " ('after', 'O'),\n",
       " ('Moslem', 'MISC-B'),\n",
       " ('Friday', 'O'),\n",
       " ('weekend', 'O'),\n",
       " (',', 'O'),\n",
       " ('brokers', 'O'),\n",
       " ('said', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "st = Sentence(16471)\n",
    "print(st.sentence)\n",
    "list(zip(st.sentence.split(' '), st.get_stored_hyperstring('annoE', tagScheme= 'BIO')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ORG': 10496, 'MISC': 5248, 'PER': 10990, 'LOC': 10944}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResumeEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# from nlptext.base import BasicObject\n",
    "\n",
    "# CORPUSPath = 'corpus/ResumeEN/'\n",
    "\n",
    "# Corpus2GroupMethod = '.json'\n",
    "\n",
    "# Group2TextMethod   = 'line'\n",
    "\n",
    "# Text2SentMethod  = 'whole'\n",
    "\n",
    "# Sent2TokenMethod = 'pos_en' # as this is CoNLL type, we don't need pos_en to seg sentences.\n",
    "# TOKENLevel = 'word'\n",
    "\n",
    "# min_token_freq = 1\n",
    "\n",
    "# use_hyper = ['pos_en']\n",
    "\n",
    "# anno = 'json_annotation'\n",
    "# anno_keywords = {\n",
    "#     'strText': 'content',\n",
    "#     'labels': 'annotation',\n",
    "# }\n",
    "\n",
    "# BasicObject.INIT(CORPUSPath, \n",
    "#                  Corpus2GroupMethod, \n",
    "#                  Group2TextMethod, \n",
    "#                  Text2SentMethod, \n",
    "#                  Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "#                  use_hyper = use_hyper, \n",
    "#                  anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPBA2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/NLPBA2004/sampletest1.iob2\n",
      "corpus/NLPBA2004/Genia4ERtask1.iob2\n",
      "Total Num of All    Tokens 456790\n",
      "Total Num of Unique Tokens 20663\n",
      "CORPUS\tit is Dumped into file: data/NLPBA2004/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/NLPBA2004/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 2\n",
      "TEXT\tit is Dumped into file: data/NLPBA2004/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 16746\n",
      "SENT\tit is Dumped into file: data/NLPBA2004/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 16746\n",
      "TOKEN\tit is Dumped into file: data/NLPBA2004/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 456790\n",
      "**************************************** \n",
      "\n",
      "pos_en-bioes\tis Dumped into file: data/NLPBA2004/word/Vocab/pos_en-bioes.voc\n",
      "pos_en-bioes\tthe length of it is   : 181\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/pos_en-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/NLPBA2004/word/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 21\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/NLPBA2004/word/Vocab/token.voc\n",
      "token   \tthe length of it is   : 20663\n",
      "\t\tWrite to: data/NLPBA2004/word/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/NLPBA2004/'\n",
    "\n",
    "Corpus2GroupMethod = '.iob2'\n",
    "\n",
    "Group2TextMethod   = 'block'\n",
    "\n",
    "Text2SentMethod  = 'whole'\n",
    "\n",
    "Sent2TokenMethod = ' '\n",
    "TOKENLevel = 'word'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos_en']\n",
    "\n",
    "anno = 'conll_block'\n",
    "anno_keywords = {\n",
    "    'anno_sep': '\\t',\n",
    "    'connector': ' ', \n",
    "    'suffix': False,\n",
    "    \"change_tags\": False, \n",
    "}\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aberrant activation of AP-1 by gp160 in CD4 positive T cells could result in up-regulation of cytokines containing AP-1 sites , e.g . interleukin-3 and granulocyte macrophage colony-stimulating factor , and concurrently lead to T cell unresponsiveness by inhibiting interleukin-2 secretion .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'O'),\n",
       " ('aberrant', 'O'),\n",
       " ('activation', 'O'),\n",
       " ('of', 'O'),\n",
       " ('AP-1', 'protein-B'),\n",
       " ('by', 'O'),\n",
       " ('gp160', 'protein-B'),\n",
       " ('in', 'O'),\n",
       " ('CD4', 'cell_type-B'),\n",
       " ('positive', 'cell_type-I'),\n",
       " ('T', 'cell_type-I'),\n",
       " ('cells', 'cell_type-I'),\n",
       " ('could', 'O'),\n",
       " ('result', 'O'),\n",
       " ('in', 'O'),\n",
       " ('up-regulation', 'O'),\n",
       " ('of', 'O'),\n",
       " ('cytokines', 'protein-B'),\n",
       " ('containing', 'O'),\n",
       " ('AP-1', 'DNA-B'),\n",
       " ('sites', 'DNA-I'),\n",
       " (',', 'O'),\n",
       " ('e.g', 'O'),\n",
       " ('.', 'O'),\n",
       " ('interleukin-3', 'protein-B'),\n",
       " ('and', 'O'),\n",
       " ('granulocyte', 'protein-B'),\n",
       " ('macrophage', 'protein-I'),\n",
       " ('colony-stimulating', 'protein-I'),\n",
       " ('factor', 'protein-I'),\n",
       " (',', 'O'),\n",
       " ('and', 'O'),\n",
       " ('concurrently', 'O'),\n",
       " ('lead', 'O'),\n",
       " ('to', 'O'),\n",
       " ('T', 'cell_type-B'),\n",
       " ('cell', 'cell_type-I'),\n",
       " ('unresponsiveness', 'O'),\n",
       " ('by', 'O'),\n",
       " ('inhibiting', 'O'),\n",
       " ('interleukin-2', 'protein-B'),\n",
       " ('secretion', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "st = Sentence(16745)\n",
    "print(st.sentence)\n",
    "\n",
    "list(zip(st.sentence.split(' '), st.get_stored_hyperstring('annoE', tagScheme= 'BIO')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'protein': 30393,\n",
       " 'cell_type': 6774,\n",
       " 'DNA': 9539,\n",
       " 'cell_line': 3832,\n",
       " 'RNA': 966}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "d = {}\n",
    "freq = []\n",
    "for locidx in range(BasicObject.SENT['length']):\n",
    "    st = Sentence(locidx)\n",
    "    tags = st.get_stored_hyperstring('annoE', tagScheme= 'BIOES')\n",
    "    SSETs = extractSET(tags)\n",
    "\n",
    "    freq.append(len(SSETs))\n",
    "    for sset in SSETs:\n",
    "        label = sset[-1]\n",
    "        if label in d:\n",
    "            d[label] = d[label] + 1\n",
    "        else:\n",
    "            d[label] = 1\n",
    "    \n",
    "print(sum([v for k, v in d.items()]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Handle Labels\n",
    "\n",
    "## strText and SSET\n",
    "\n",
    "For each text, we must get a strText and SSET, which meet the following restriction.\n",
    "\n",
    "Besides, we also need a strSents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结肠多发息肉。\n",
      "患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('多发息肉', 2, 6, '疾病'),\n",
       " ('慢性', 15, 17, '修饰'),\n",
       " ('多发息肉', 29, 33, '疾病'),\n",
       " ('3月余', 33, 36, '修饰'),\n",
       " ('无阳性体征', 43, 48, '不确定')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "print(strText)\n",
    "\n",
    "strAnnoText = '''标注文本名称:/Users/zhangling/Documents/新标的数据530/529李选-已检查/Entity/patient4378.txt\\n标注文本字数统计:87\\n多发息肉\\t3\\t6\\t疾病\\n慢性\\t16\\t17\\t修饰\\n多发息肉\\t30\\t33\\t疾病\\n3月余\\t34\\t36\\t修饰\\n无阳性体征\\t44\\t48\\t不确定\\n'''\n",
    "\n",
    "# BIOES\n",
    "\n",
    "sep = '\\t'\n",
    "SSETText = [sset.split('\\t') for sset in strAnnoText.split('\\n') if sep in sset]\n",
    "\n",
    "\n",
    "notZeroIndex = 1 \n",
    "for idx, sset in enumerate(SSETText):\n",
    "    data = (sset[0], int(sset[1]) - notZeroIndex, int(sset[2]), sset[-1])\n",
    "    SSETText[idx] = data\n",
    "\n",
    "\n",
    "# check\n",
    "for sset in SSETText:\n",
    "    try:\n",
    "        assert strText[sset[1]: sset[2]] == sset[0]\n",
    "    except:\n",
    "        print('strText:', strText[sset[1] : sset[2]])\n",
    "        print('SSETText:', sset[0])\n",
    "        \n",
    "SSETText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCITText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['结', 0, 'O'],\n",
       " ['肠', 1, 'O'],\n",
       " ['多', 2, '疾病-B'],\n",
       " ['发', 3, '疾病-I'],\n",
       " ['息', 4, '疾病-I'],\n",
       " ['肉', 5, '疾病-E'],\n",
       " ['。', 6, 'O'],\n",
       " ['\\n', 7, 'O'],\n",
       " ['患', 8, 'O'],\n",
       " ['中', 9, 'O'],\n",
       " ['老', 10, 'O'],\n",
       " ['年', 11, 'O'],\n",
       " ['男', 12, 'O'],\n",
       " ['性', 13, 'O'],\n",
       " [',', 14, 'O'],\n",
       " ['慢', 15, '修饰-B'],\n",
       " ['性', 16, '修饰-E'],\n",
       " ['病', 17, 'O'],\n",
       " ['程', 18, 'O'],\n",
       " ['。', 19, 'O'],\n",
       " [' ', 20, 'O'],\n",
       " ['因', 21, 'O'],\n",
       " ['“', 22, 'O'],\n",
       " ['体', 23, 'O'],\n",
       " ['检', 24, 'O'],\n",
       " ['发', 25, 'O'],\n",
       " ['现', 26, 'O'],\n",
       " ['大', 27, 'O'],\n",
       " ['肠', 28, 'O'],\n",
       " ['多', 29, '疾病-B'],\n",
       " ['发', 30, '疾病-I'],\n",
       " ['息', 31, '疾病-I'],\n",
       " ['肉', 32, '疾病-E'],\n",
       " ['3', 33, '修饰-B'],\n",
       " ['月', 34, '修饰-I'],\n",
       " ['余', 35, '修饰-E'],\n",
       " ['”', 36, 'O'],\n",
       " ['入', 37, 'O'],\n",
       " ['院', 38, 'O'],\n",
       " ['。', 39, 'O'],\n",
       " ['查', 40, 'O'],\n",
       " ['体', 41, 'O'],\n",
       " [':', 42, 'O'],\n",
       " ['无', 43, '不确定-B'],\n",
       " ['阳', 44, '不确定-I'],\n",
       " ['性', 45, '不确定-I'],\n",
       " ['体', 46, '不确定-I'],\n",
       " ['征', 47, '不确定-E'],\n",
       " ['。', 48, 'O']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "strText = '''结肠多发息肉。\\n患中老年男性,慢性病程。 因“体检发现大肠多发息肉3月余”入院。查体:无阳性体征。'''\n",
    "\n",
    "\n",
    "SSETText = [ ('多发息肉', 2, 6, '疾病'),\n",
    "             ('慢性', 15, 17, '修饰'),\n",
    "             ('多发息肉', 29, 33, '疾病'),\n",
    "             ('3月余', 33, 36, '修饰'),\n",
    "             ('无阳性体征', 43, 48, '不确定')]\n",
    "\n",
    "def getCITText(strText, SSETText, TOKENLevel='char'):\n",
    "    len(SSETText) > 0 \n",
    "    if TOKENLevel == 'char':\n",
    "        for sset in SSETText:\n",
    "            try:\n",
    "                assert strText[sset[1]: sset[2]] == sset[0]\n",
    "            except:\n",
    "                print('strText:', strText[sset[1] : sset[2]])\n",
    "                print('SSETText:', sset[0])\n",
    "        CITAnnoText = []\n",
    "        for sset in SSETText:\n",
    "            # BIOES\n",
    "            strAnno, s, e, tag = sset\n",
    "            CIT = [[c, s + idx, tag+ '-I']  for idx, c in enumerate(strAnno)]\n",
    "            CIT[-1][2] = tag + '-E'\n",
    "            CIT[ 0][2] = tag + '-B'\n",
    "            if len(CIT) == 1:\n",
    "                CIT[0][2] = tag + '-S' \n",
    "            CITAnnoText.extend(CIT)\n",
    "\n",
    "        # print(strAnnoText)\n",
    "        CITText = [[char, idx, 'O'] for idx, char in enumerate(strText)]\n",
    "        for citAnno in CITAnnoText:\n",
    "            c, idx, t = citAnno\n",
    "            assert CITText[idx][0] == c\n",
    "            CITText[idx] = citAnno\n",
    "\n",
    "    elif TOKENLevel == 'word':\n",
    "        CITText = []\n",
    "        for idx, sset in enumerate(SSETText):\n",
    "            try:\n",
    "                assert sset[0] == strText[idx]\n",
    "            except:\n",
    "                print(strText)[idx]\n",
    "                print(sset[0])\n",
    "\n",
    "            CITText.append(sset)\n",
    "    return CITText\n",
    "\n",
    "\n",
    "CITText = getCITText(strText, SSETText, TOKENLevel='char')\n",
    "CITText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCITSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['结', 0, 'O'],\n",
       "  ['肠', 1, 'O'],\n",
       "  ['多', 2, '疾病-B'],\n",
       "  ['发', 3, '疾病-I'],\n",
       "  ['息', 4, '疾病-I'],\n",
       "  ['肉', 5, '疾病-E'],\n",
       "  ['。', 6, 'O']],\n",
       " [['患', 0, 'O'],\n",
       "  ['中', 1, 'O'],\n",
       "  ['老', 2, 'O'],\n",
       "  ['年', 3, 'O'],\n",
       "  ['男', 4, 'O'],\n",
       "  ['性', 5, 'O'],\n",
       "  [',', 6, 'O'],\n",
       "  ['慢', 7, '修饰-B'],\n",
       "  ['性', 8, '修饰-E'],\n",
       "  ['病', 9, 'O'],\n",
       "  ['程', 10, 'O'],\n",
       "  ['。', 11, 'O']],\n",
       " [['因', 0, 'O'],\n",
       "  ['“', 1, 'O'],\n",
       "  ['体', 2, 'O'],\n",
       "  ['检', 3, 'O'],\n",
       "  ['发', 4, 'O'],\n",
       "  ['现', 5, 'O'],\n",
       "  ['大', 6, 'O'],\n",
       "  ['肠', 7, 'O'],\n",
       "  ['多', 8, '疾病-B'],\n",
       "  ['发', 9, '疾病-I'],\n",
       "  ['息', 10, '疾病-I'],\n",
       "  ['肉', 11, '疾病-E'],\n",
       "  ['3', 12, '修饰-B'],\n",
       "  ['月', 13, '修饰-I'],\n",
       "  ['余', 14, '修饰-E'],\n",
       "  ['”', 15, 'O'],\n",
       "  ['入', 16, 'O'],\n",
       "  ['院', 17, 'O'],\n",
       "  ['。', 18, 'O']],\n",
       " [['查', 0, 'O'],\n",
       "  ['体', 1, 'O'],\n",
       "  [':', 2, 'O'],\n",
       "  ['无', 3, '不确定-B'],\n",
       "  ['阳', 4, '不确定-I'],\n",
       "  ['性', 5, '不确定-I'],\n",
       "  ['体', 6, '不确定-I'],\n",
       "  ['征', 7, '不确定-E'],\n",
       "  ['。', 8, 'O']]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strSents = ['结肠多发息肉。', '患中老年男性,慢性病程。', '因“体检发现大肠多发息肉3月余”入院。', '查体:无阳性体征。']\n",
    "\n",
    "CITText  =  [['结', 0, 'O'],\n",
    "             ['肠', 1, 'O'],\n",
    "             ['多', 2, '疾病-B'],\n",
    "             ['发', 3, '疾病-I'],\n",
    "             ['息', 4, '疾病-I'],\n",
    "             ['肉', 5, '疾病-E'],\n",
    "             ['。', 6, 'O'],\n",
    "             ['\\n', 7, 'O'],\n",
    "             ['患', 8, 'O'],\n",
    "             ['中', 9, 'O'],\n",
    "             ['老', 10, 'O'],\n",
    "             ['年', 11, 'O'],\n",
    "             ['男', 12, 'O'],\n",
    "             ['性', 13, 'O'],\n",
    "             [',', 14, 'O'],\n",
    "             ['慢', 15, '修饰-B'],\n",
    "             ['性', 16, '修饰-E'],\n",
    "             ['病', 17, 'O'],\n",
    "             ['程', 18, 'O'],\n",
    "             ['。', 19, 'O'],\n",
    "             [' ', 20, 'O'],\n",
    "             ['因', 21, 'O'],\n",
    "             ['“', 22, 'O'],\n",
    "             ['体', 23, 'O'],\n",
    "             ['检', 24, 'O'],\n",
    "             ['发', 25, 'O'],\n",
    "             ['现', 26, 'O'],\n",
    "             ['大', 27, 'O'],\n",
    "             ['肠', 28, 'O'],\n",
    "             ['多', 29, '疾病-B'],\n",
    "             ['发', 30, '疾病-I'],\n",
    "             ['息', 31, '疾病-I'],\n",
    "             ['肉', 32, '疾病-E'],\n",
    "             ['3', 33, '修饰-B'],\n",
    "             ['月', 34, '修饰-I'],\n",
    "             ['余', 35, '修饰-E'],\n",
    "             ['”', 36, 'O'],\n",
    "             ['入', 37, 'O'],\n",
    "             ['院', 38, 'O'],\n",
    "             ['。', 39, 'O'],\n",
    "             ['查', 40, 'O'],\n",
    "             ['体', 41, 'O'],\n",
    "             [':', 42, 'O'],\n",
    "             ['无', 43, '不确定-B'],\n",
    "             ['阳', 44, '不确定-I'],\n",
    "             ['性', 45, '不确定-I'],\n",
    "             ['体', 46, '不确定-I'],\n",
    "             ['征', 47, '不确定-E'],\n",
    "             ['。', 48, 'O']]\n",
    "\n",
    "def getCITSents(strSents, CITText):\n",
    "    lenLastSent = 0\n",
    "    collapse    = 0 # don't need to move \n",
    "    CITSents = []\n",
    "    for strSent in strSents:\n",
    "        CITSent = []\n",
    "        for sentTokenIdx, c in enumerate(strSent):\n",
    "            # sentTokenIdx = txtTokenIdx - lenLastSent - collapse\n",
    "            txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "            cT, _, tT = CITText[txtTokenIdx]\n",
    "            while c != cT and c != ' ':\n",
    "                collapse = collapse + 1\n",
    "                txtTokenIdx = sentTokenIdx + lenLastSent + collapse\n",
    "                cT, _, tT = CITText[txtTokenIdx]\n",
    "            CITSent.append([c,sentTokenIdx, tT])\n",
    "        lenLastSent = lenLastSent + len(strSent)\n",
    "        CITSents.append(CITSent)\n",
    "    # CITSents\n",
    "    # Here we get CITSents  \n",
    "    return CITSents\n",
    "       \n",
    "    \n",
    "CITSents = getCITSents(strSents, CITText)\n",
    "CITSents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getSSET_from_CIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['结', 0, 'O'], ['肠', 1, 'O'], ['多', 2, '疾病-B'], ['发', 3, '疾病-I'], ['息', 4, '疾病-I'], ['肉', 5, '疾病-E'], ['。', 6, 'O']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O', 'O', '疾病-B', '疾病-I', '疾病-I', '疾病-E', 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CITSent = CITSents[0]\n",
    "print(CITSent)\n",
    "tag_seq = [i[-1] for i in CITSent]\n",
    "tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSSET_from_CIT(orig_seq = None, tag_seq = None, CIT = None, tag_seq_tagScheme = 'BIO', join_char = ''):\n",
    "    # orig_seq is sentence without start or end\n",
    "    # tag_seq may have start or end\n",
    "        \n",
    "    tagScheme = tag_seq_tagScheme\n",
    "    if tagScheme == 'BIOES':\n",
    "        tag_seq = [i.replace('-S', '-B').replace('-E', '-I') for i in tag_seq]\n",
    "    elif tagScheme == 'BIOE':\n",
    "        tag_seq = [i.replace('-E', '-I') for i in tag_seq]\n",
    "    elif tagScheme == 'BIOS':\n",
    "        tag_seq = [i.replace('-S', '-B') for i in tag_seq]\n",
    "    elif tagScheme == 'BIO':\n",
    "        pass\n",
    "    else:\n",
    "        print('The tagScheme', tagScheme, 'is not supported yet...')\n",
    "    \n",
    "    if not CIT:\n",
    "        # use BIO tagScheme\n",
    "        CIT = list(zip(orig_seq, range(len(orig_seq)), tag_seq))\n",
    "    taggedCIT = [cit for cit in CIT if cit[2]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedCIT)) if taggedCIT[idx][2][-2:] == '-B']\n",
    "    startIdx.append(len(taggedCIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedCIT[startIdx[i]: startIdx[i+1]]\n",
    "        string = join_char.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][1], entityAtom[-1][1] + 1\n",
    "        tag = entityAtom[0][2].split('-')[0]\n",
    "        entitiesList.append((string, start, end, tag))\n",
    "    return entitiesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get strText, strSents, and SSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
